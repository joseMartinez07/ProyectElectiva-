{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Importar la biblioteca pandas para manipulación de datos"
      ],
      "metadata": {
        "id": "3Qw7NI4UT1-A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carga y análisis preliminar** específicamente enfocado en la identificación de valores faltantes."
      ],
      "metadata": {
        "id": "i3KLhimIULFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZoVHGj9wR31D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284c501a-5bed-49ee-c669-44bd5d588ab4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index       0\n",
              "Gender      0\n",
              "Place       0\n",
              "Location    0\n",
              "Date        0\n",
              "Type        0\n",
              "Label       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Carga del conjunto de datos desde un archivo CSV\n",
        "df = pd.read_csv('/content/rest-mex_2022_recommendation_data_training.csv')\n",
        "\n",
        "# Revisión de valores faltantes en el DataFrame\n",
        "# El método isnull() identifica los valores nulos, y sum() suma estos valores por columna\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Imprimir los resultados de la revisión de valores faltantes\n",
        "# Cada columna del DataFrame se lista con el número de valores nulos encontrados\n",
        "missing_values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** El resultado muestra que no hay valores faltantes en ninguna de las columnas del DataFrame. Las columnas listadas incluyen Index, Gender, Place, Location, Date, Type, y Label, todas con un conteo de 0 valores faltantes."
      ],
      "metadata": {
        "id": "W8TBA1nJMwO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verificación de los valores** únicos presentes en la columna 'Gender' de un DataFrame de pandas."
      ],
      "metadata": {
        "id": "zkURtdTxM1mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación de valores únicos en la columna 'Gender'\n",
        "# Esto ayuda a entender qué categorías de género están presentes en el conjunto de datos\n",
        "gender_values = df['Gender'].unique()\n",
        "\n",
        "# Imprimir los valores únicos encontrados en la columna 'Gender'\n",
        "# Se espera ver categorías como 'Male', 'Female', y posiblemente otras como 'N/I' para no informado\n",
        "gender_values"
      ],
      "metadata": {
        "id": "6DpEm1DtUR3r",
        "outputId": "6b298821-8aa7-49e4-cbf8-334ce2467631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Male', 'Female', 'N/I'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**El método describe(include='all')** se utiliza para generar un resumen estadístico del DataFrame df. Este método proporciona una visión general de las estadísticas descriptivas que incluye:\n",
        "\n",
        "count: número de entradas no nulas.\n",
        "unique: número de valores únicos.\n",
        "top: el valor más común.\n",
        "freq: la frecuencia del valor más común.\n",
        "mean, std, min, 25%, 50%, 75%, max: estadísticas descriptivas para columnas numéricas.\n",
        "Al usar include='all', el método incluye columnas de todos los tipos de datos."
      ],
      "metadata": {
        "id": "qilZ4-k6N1Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resumen estadístico y verificación de valores faltantes\n",
        "\n",
        "# Generar un resumen estadístico de todas las columnas del DataFrame\n",
        "# Incluye estadísticas descriptivas para columnas numéricas y categóricas\n",
        "summary = df.describe(include='all')\n",
        "\n",
        "# Contar y verificar la presencia de valores nulos en cada columna del DataFrame\n",
        "# Esencial para la limpieza de datos y análisis precisos\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Imprimir el resumen estadístico y el conteo de valores nulos\n",
        "summary, missing_values\n"
      ],
      "metadata": {
        "id": "ifgbut3pVMRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior**\n",
        "El resumen estadístico muestra detalles como el número total de registros (count), la categoría más frecuente (top) y su frecuencia (freq) para cada columna. Para columnas numéricas, muestra estadísticas adicionales como la media (mean), desviación estándar (std),"
      ],
      "metadata": {
        "id": "SrkWlAZiOJiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Procesar y transformar datos** de fechas, específicamente ajustando las fechas a un formato de año y mes."
      ],
      "metadata": {
        "id": "bd_lsyRCPdjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Ruta al archivo CSV con los datos de recomendaciones\n",
        "output_file_path=('/content/rest-mex_2022_recommendation_data_training.csv')\n",
        "\n",
        "# Carga del archivo CSV en un DataFrame\n",
        "df = pd.read_csv(output_file_path)\n",
        "\n",
        "# Función para convertir fechas del formato texto a formato año-mes\n",
        "def convert_to_year_month_format(date_str):\n",
        "    # Mapeo de los meses en español a sus números correspondientes\n",
        "    months_map = {\n",
        "        \"enero\": \"01\",\n",
        "        \"febrero\": \"02\",\n",
        "        \"marzo\": \"03\",\n",
        "        \"abril\": \"04\",\n",
        "        \"mayo\": \"05\",\n",
        "        \"junio\": \"06\",\n",
        "        \"julio\": \"07\",\n",
        "        \"agosto\": \"08\",\n",
        "        \"septiembre\": \"09\",\n",
        "        \"octubre\": \"10\",\n",
        "        \"noviembre\": \"11\",\n",
        "        \"diciembre\": \"12\"\n",
        "    }\n",
        "\n",
        "    # Extracción del mes y año\n",
        "    match = re.search(r\"(\\w+) de (\\d{4})\", date_str)\n",
        "    if match:\n",
        "        month_spanish, year = match.groups()\n",
        "        month_number = months_map.get(month_spanish.lower())\n",
        "        return f\"{year}-{month_number}\"\n",
        "    else:\n",
        "        return None  # Mantener el valor como None si no se puede convertir\n",
        "\n",
        "# Aplicando la conversión al DataFrame\n",
        "df['Date'] = df['Date'].apply(convert_to_year_month_format)\n",
        "\n",
        "# Verificando los cambios realizados\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eSlIrDSjgJ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# se realiza una copia del Dataset a utilizar"
      ],
      "metadata": {
        "id": "9jICSyNRQQxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creando un nuevo DataFrame a partir del DataFrame actual\n",
        "new_dataframe = df.copy()\n",
        "\n",
        "# Guardando el nuevo DataFrame como un archivo CSV\n",
        "output_file_path = '/content/cleaned_recommendation_data.csv'\n",
        "new_dataframe.to_csv(output_file_path, index=False,sep=';')\n",
        "\n",
        "output_file_path"
      ],
      "metadata": {
        "id": "yLWbNQOpgs49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Codgio de limpieza y normalizacion para el archivo Places\n"
      ],
      "metadata": {
        "id": "x4eBK35HiTJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_places = pd.read_csv('/content/Places.csv') # cargue del documento Places\n",
        "df_places.head()"
      ],
      "metadata": {
        "id": "LlY0xZTGjoPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concatena todas las columnas** del dataset, excepto la primera, para formar una nueva columna llamada 'Description'. Esta operación se realiza fila por fila, uniendo los valores de las columnas con el separador ' | '."
      ],
      "metadata": {
        "id": "PLSjYclZkydI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del archivo CSV que contiene los datos de los lugares\n",
        "file_path = '/content/Places.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Creación de la columna 'Description' concatenando todas las columnas excepto la primera\n",
        "# Se utiliza el separador ' | ' para unir los valores\n",
        "data['Description'] = data.iloc[:, 1:].apply(lambda x: ' | '.join(x.dropna()), axis=1)\n",
        "\n",
        "# Creación de un nuevo DataFrame con solo las columnas 'Lugar' y 'Description'\n",
        "df_places = data[['Lugar', 'Description']]\n",
        "\n",
        "# Visualización de las primeras filas del DataFrame transformado\n",
        "df_places.head()\n"
      ],
      "metadata": {
        "id": "hueGzw0OnBmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Reemplazo de Caracteres en la Columna 'Description': El script reemplaza todos los caracteres '|' por comas (',') en la columna 'Description'. Esto cambia el formato de la descripción, haciendo que los diferentes elementos estén separados por comas en lugar de por el separador original ' | '."
      ],
      "metadata": {
        "id": "fOi0yPbGqFTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modificación del Formato de Descripción:** El código reemplaza todos los caracteres '|' (barras verticales) por comas (',') en la columna 'Description'. Este cambio tiene como objetivo mejorar la legibilidad y el formato de las descripciones en el DataFrame."
      ],
      "metadata": {
        "id": "EcmQ2t_pTIdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de una copia independiente del DataFrame para evitar modificar el original\n",
        "df_places = df_places.copy()\n",
        "\n",
        "# Reemplazo de todos los separadores '|' por comas ',' en la columna 'Description'\n",
        "# Esto se hace para mejorar la legibilidad y formato de las descripciones\n",
        "df_places['Description'] = df_places['Description'].str.replace('|', ',', regex=False)\n",
        "\n",
        "# Visualización de las primeras filas del DataFrame para confirmar los cambios\n",
        "# Esto ayuda a verificar que los separadores han sido reemplazados correctamente\n",
        "df_places.head()\n"
      ],
      "metadata": {
        "id": "5wG-kGmeqNh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Muestra una transformación exitosa"
      ],
      "metadata": {
        "id": "31cheWMqTaFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con estos cambios, las descripciones de los lugares en el DataFrame df_places ahora presentan una separación más estándar y posiblemente más legible, utilizando comas en lugar de barras verticales."
      ],
      "metadata": {
        "id": "39X8jVARtumI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exportación de datos del DataFrame** df_places a un nuevo archivo CSV con algunas modificaciones específicas"
      ],
      "metadata": {
        "id": "_T1GvLsLqOF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creando un nuevo DataFrame a partir del DataFrame actual para mantener los datos originales intactos\n",
        "new_dataframe = df_places.copy()\n",
        "\n",
        "# Especificando la ruta del archivo de salida para el nuevo DataFrame\n",
        "output_places_file_path = '/content/Places_semicolon_delimited.csv'\n",
        "\n",
        "# Guardando el DataFrame en un archivo CSV utilizando punto y coma como delimitador\n",
        "# El uso de ';' ayuda a evitar problemas si los campos contienen comas\n",
        "df_places.to_csv(output_places_file_path, sep=';', index=False)\n",
        "\n",
        "# Mostrando la ruta del archivo de salida para confirmar su creación\n",
        "output_places_file_path"
      ],
      "metadata": {
        "id": "Wf-Fe8WoiNVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_places.head()"
      ],
      "metadata": {
        "id": "LQ7mecQ2rjBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Termina normalizacion y limpieza del archivo places\n"
      ],
      "metadata": {
        "id": "fNYB8bIfsUQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combinar dos conjuntos de datos** relacionados con recomendaciones de lugares, places y rest_mex"
      ],
      "metadata": {
        "id": "iTG1vS7tvKGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de los dos datasets\n",
        "cleaned_recommendation_data_path = '/content/cleaned_recommendation_data.csv'\n",
        "places_semicolon_delimited_path = '/content/Places_semicolon_delimited.csv'\n",
        "\n",
        "# Lectura de los archivos CSV\n",
        "# Se especifica el separador ';' para el archivo cleaned_recommendation_data\n",
        "cleaned_recommendation_data = pd.read_csv(cleaned_recommendation_data_path, sep=';')\n",
        "places_semicolon_delimited = pd.read_csv(places_semicolon_delimited_path, sep=';')\n",
        "\n",
        "# Renombrar la columna 'Lugar' a 'Place' en places_semicolon_delimited para que coincida con cleaned_recommendation_data\n",
        "places_semicolon_delimited.rename(columns={'Lugar': 'Place'}, inplace=True)\n",
        "\n",
        "# Fusión de los dos datasets usando la columna 'Place'\n",
        "# Se utiliza un join externo (outer) para incluir todas las filas, incluso si no hay coincidencia\n",
        "merged_data = pd.merge(cleaned_recommendation_data, places_semicolon_delimited, on='Place', how='outer')\n",
        "\n",
        "# Mostrar las primeras filas del dataset combinado\n",
        "merged_data.head()\n"
      ],
      "metadata": {
        "id": "qIPjxz-dvNcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** La salida muestra el éxito en la combinación de dos conjuntos de datos relacionados con recomendaciones y descripciones de lugares. La fusión de datos de esta manera permite una visión más integral y enriquecida, donde cada lugar recomendado se acompaña de una descripción detallada.\n",
        "\n",
        "La elección de un join externo es significativa, ya que permite conservar todos los registros de ambos conjuntos de datos, incluso aquellos que no tienen una correspondencia directa, lo que es esencial para análisis exhaustivos y completos."
      ],
      "metadata": {
        "id": "IZ7PZAtyVJjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gestiona la exportación del DataFrame combinado** merged_data a un archivo CSV."
      ],
      "metadata": {
        "id": "DDLsdNMFVh2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creando una copia del DataFrame combinado para operaciones adicionales\n",
        "new_dataframe = merged_data.copy()\n",
        "\n",
        "# Definiendo la ruta del archivo de salida para el DataFrame combinado\n",
        "output_places_file_path = '/content/merged_data_delimited.csv'\n",
        "\n",
        "# Exportando el DataFrame a un archivo CSV, usando ';' como delimitador\n",
        "# Este formato es útil para datos que contienen comas en sus campos\n",
        "new_dataframe.to_csv(output_places_file_path, sep=';', index=False)\n",
        "\n",
        "# Mostrando la ruta del archivo para confirmar la creación y ubicación del archivo CSV\n",
        "output_places_file_path"
      ],
      "metadata": {
        "id": "VrnQmbn6v1rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** La exportación del DataFrame merged_data a un archivo CSV con un delimitador de punto y coma representa un paso esencial en la gestión y compartición de grandes conjuntos de datos. El uso de un delimitador diferente a la coma convencional permite una mayor flexibilidad, especialmente útil en escenarios donde los datos incluyen comas dentro de sus campos."
      ],
      "metadata": {
        "id": "EDku_2hVV80i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inicio limpieza y union de los comentarios de los usuarios**\n"
      ],
      "metadata": {
        "id": "WbQWdsi51xCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_users = pd.read_csv('/content/Users/Usuario0.csv')\n",
        "\n",
        "df_users.head()"
      ],
      "metadata": {
        "id": "Y1Xdzx5811bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combina múltiples archivos CSV**, cada uno representando los comentarios y puntuaciones de un usuario, en un único DataFrame. Itera a través de los archivos en una carpeta específica, extrayendo el nombre de usuario de cada archivo, seleccionando ciertas columnas, y luego los concatena en un DataFrame consolidado. Finalmente, el DataFrame combinado se guarda en un nuevo archivo CSV delimitado por punto y coma."
      ],
      "metadata": {
        "id": "LPpyaJfyWdmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Reescribir el código completo para combinar los archivos CSV\n",
        "\n",
        "# Ruta a la carpeta que contiene los archivos CSV\n",
        "folder_path = '/content/Users/'\n",
        "\n",
        "# Inicialización de un DataFrame vacío para almacenar los datos combinados\n",
        "combined_data = pd.DataFrame(columns=['Usuario', 'Comentario', 'Puntuacion', 'Lugar', 'Puntuacion Global'])\n",
        "\n",
        "# Iteración a través de cada archivo en la carpeta\n",
        "for file in os.listdir(folder_path):\n",
        "    # Verificación de que el archivo sea un archivo CSV\n",
        "    if file.endswith('.csv'):\n",
        "        # Extracción del nombre de usuario (sin la extensión del archivo)\n",
        "        username = file.split('.')[0]\n",
        "\n",
        "         # Lectura del archivo CSV, seleccionando solo las primeras cuatro columnas\n",
        "        df = pd.read_csv(os.path.join(folder_path, file), header=None, usecols=[0, 1, 2, 3])\n",
        "\n",
        "        # Añadiendo una nueva columna para el nombre de usuario\n",
        "        df['Usuario'] = username\n",
        "\n",
        "        # Renombrando las columnas\n",
        "        df.columns = ['Comentario', 'Puntuacion', 'Lugar', 'Puntuacion Global', 'Usuario']\n",
        "\n",
        "        # Reordenando las columnas\n",
        "        df = df[['Usuario', 'Comentario', 'Puntuacion', 'Lugar', 'Puntuacion Global']]\n",
        "\n",
        "        # Añadiendo estos datos al DataFrame combinado\n",
        "        combined_data = pd.concat([combined_data, df])\n",
        "\n",
        "# Reiniciando el índice del DataFrame combinado\n",
        "combined_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Guardando los datos combinados en un nuevo archivo CSV\n",
        "output_file_path = '/content/combined_data.csv'\n",
        "combined_data.to_csv(output_file_path, index=False,sep=';')\n",
        "\n",
        "# Mostrando la ruta del archivo para confirmar la creación y ubicación del archivo CSV\n",
        "output_file_path"
      ],
      "metadata": {
        "id": "1NcBw5iR3ysk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** El script realiza con éxito la tarea de combinar múltiples archivos CSV en un solo conjunto de datos, proporcionando una visión unificada de las interacciones de los usuarios. Esta consolidación de datos es fundamental en análisis de datos y en escenarios donde se requiere una vista agregada de información proveniente de múltiples fuentes."
      ],
      "metadata": {
        "id": "kavEgLuWXdYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cuenta la cantidad de archivos en una carpeta específica**, proporcionando un recuento de los archivos CSV (o cualquier otro tipo de archivo) contenidos en ella"
      ],
      "metadata": {
        "id": "x9KMhuFBYlwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta a la carpeta que contiene los archivos CSV\n",
        "folder_path = '/content/Users/'\n",
        "\n",
        "# Conteo del número de archivos en la carpeta\n",
        "# Se incluyen solo los archivos, excluyendo subdirectorios\n",
        "file_count = len([name for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))])\n",
        "\n",
        "# Mostrando el recuento de archivos\n",
        "file_count"
      ],
      "metadata": {
        "id": "yllfiaL7LA5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Se determina que hay 1311 archivos en la carpeta especificada"
      ],
      "metadata": {
        "id": "nPnfcsHmY0OM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Union del primer merge y el csv de todos los comentarios de todos los usuarios**"
      ],
      "metadata": {
        "id": "Mv42uODoGZLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combina dos datasets distintos,** uno que contiene datos combinados de usuarios y otro con datos fusionados sobre lugares, en un único DataFrame. Utiliza un método de fusión basado en ciertas columnas clave, y maneja líneas mal formateadas en los archivos CSV al leerlos."
      ],
      "metadata": {
        "id": "Ang3sN1LZlSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rutas a los archivos CSV\n",
        "combined_data_path = '/content/combined_data.csv'\n",
        "merged_data_delimited_path = '/content/merged_data_delimited.csv'\n",
        "\n",
        "# Lectura del archivo combined_data.csv, omitiendo líneas mal formateadas\n",
        "combined_data = pd.read_csv(combined_data_path, delimiter=';', on_bad_lines='skip')\n",
        "\n",
        "# Lectura del archivo merged_data_delimited.csv, con el delimitador correcto y omitiendo líneas mal formateadas\n",
        "merged_data_delimited = pd.read_csv(merged_data_delimited_path, delimiter=';', on_bad_lines='skip')\n",
        "\n",
        "# Fusión de los datasets basada en las columnas 'Index' y 'Place' de un dataset y 'Usuario' y 'Lugar' del otro\n",
        "merged_data = pd.merge(merged_data_delimited, combined_data, left_on=['Index', 'Place'], right_on=['Usuario', 'Lugar'])\n",
        "\n",
        "# Muestra las primeras filas del dataset fusionado\n",
        "print(merged_data.head())\n",
        "\n",
        "# Guardado del dataset fusionado en un nuevo archivo\n",
        "merged_data.to_csv('/content/merged_data.csv', index=False, sep=';')\n"
      ],
      "metadata": {
        "id": "cIR65TPBGR8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** muestra una fusión exitosa de los dos datasets"
      ],
      "metadata": {
        "id": "T3mWJ41ZZ7sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = pd.read_csv('/content/merged_data.csv', delimiter=';')  # Replace ';' with the correct delimiter\n",
        "merged_data.head()\n"
      ],
      "metadata": {
        "id": "NY2qmIucSPa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limpieza para el datset Resultante de la Union,** se buscan caracteres extraños."
      ],
      "metadata": {
        "id": "ZwygS446MAS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "# Función para detectar caracteres extraños\n",
        "def detectar_caracteres_extranos(texto):\n",
        "    caracteres_extranos = []\n",
        "    for caracter in texto:\n",
        "        if ord(caracter) > 127:\n",
        "            caracteres_extranos.append(caracter)\n",
        "    return caracteres_extranos\n",
        "\n",
        "# Función para limpiar texto\n",
        "def limpiar_texto(texto):\n",
        "    texto_normalizado = unicodedata.normalize('NFKC', texto)\n",
        "    return texto_normalizado\n",
        "\n",
        "# Cargando el dataset\n",
        "file_path = '/content/merged_data.csv'\n",
        "dataset = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Revisando caracteres extraños\n",
        "columnas_revisar = ['Description', 'Comentario']\n",
        "caracteres_extranos_unicos = set()\n",
        "\n",
        "for columna in columnas_revisar:\n",
        "    for texto in dataset[columna].dropna():\n",
        "        caracteres_extranos = detectar_caracteres_extranos(texto)\n",
        "        caracteres_extranos_unicos.update(caracteres_extranos)\n",
        "\n",
        "# Limpiando el dataset\n",
        "for columna in columnas_revisar:\n",
        "    dataset[columna] = dataset[columna].astype(str).apply(limpiar_texto)\n",
        "\n",
        "# Guardando el dataset limpio\n",
        "new_file_path = '/content/merged_data_cleaned.csv'\n",
        "dataset.to_csv(new_file_path, sep=';', index=False)\n",
        "\n",
        "# Ruta del archivo guardado\n",
        "print(f'Archivo guardado: {new_file_path}')\n",
        "print(dataset.head())"
      ],
      "metadata": {
        "id": "4otq0GcIU09r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargando el dataset\n",
        "file_path = '/content/merged_data_cleaned.csv'\n",
        "dataset_clean = pd.read_csv(file_path, delimiter=';')\n",
        "print(dataset_clean)"
      ],
      "metadata": {
        "id": "7m4coIkUWvAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procesa un dataset de datos combinados,** eliminando columnas innecesarias y aplicando una función para etiquetar a los visitantes como 'Mexicano' o 'Extranjero' basándose en su ubicación."
      ],
      "metadata": {
        "id": "9GJwnTCube6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el archivo\n",
        "file_path = '/content/merged_data_cleaned.csv'\n",
        "df = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Eliminar la columna 'Usuario'\n",
        "df.drop('Usuario', axis=1, inplace=True)\n",
        "df.drop('Lugar', axis=1, inplace=True)\n",
        "\n",
        "# Función para determinar si es extranjero o mexicano\n",
        "# Modificando la función 'etiquetar' según las nuevas instrucciones\n",
        "def etiquetar_modificada(location):\n",
        "    # Lista de ubicaciones que clasificarán como 'Mexicano'\n",
        "    ubicaciones_mexicanas = ['noreste', 'noroeste', 'centro', 'occidente', 'sureste']\n",
        "\n",
        "    # Comprobar si la ubicación está en la lista\n",
        "    if location.lower() in ubicaciones_mexicanas:\n",
        "        return 'Mexicano'\n",
        "    else:\n",
        "        return 'Extranjero'\n",
        "\n",
        "# Aplicar la función modificada para crear la columna 'Etiqueta'\n",
        "df['Etiqueta'] = df['Location'].apply(etiquetar_modificada)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame actualizado\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "lb8KqOElW7vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Muestra el DataFrame df después de haber eliminado las columnas 'Usuario' y 'Lugar', y haber añadido la columna 'Etiqueta', que clasifica a los visitantes como 'Mexicano' o 'Extranjero' según su ubicación."
      ],
      "metadata": {
        "id": "9qu-oOLHbxLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guardar el dataframe modificado en un nuevo archivo CSV**"
      ],
      "metadata": {
        "id": "ZakLaYpOb20j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el dataframe modificado en un nuevo archivo CSV\n",
        "modified_file_path = '/content/modified_dataset.csv'\n",
        "df.to_csv(modified_file_path, sep=';', index=False)"
      ],
      "metadata": {
        "id": "zUQrUd47bErD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vUc_yY-qMDEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo de análisis de sentimientos basado en BERT** para evaluar el tono emocional de los comentarios en un dataset. Carga un modelo pre-entrenado de Hugging Face y lo aplica a los comentarios en el DataFrame, asignando una puntuación de sentimiento a cada comentario. Las puntuaciones son normalizadas para ajustarse a un rango de -1 a 1."
      ],
      "metadata": {
        "id": "3ji0dBXzb-eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Cargar tu archivo CSV\n",
        "df = pd.read_csv('/content/modified_dataset.csv', delimiter=';')\n",
        "\n",
        "# Cargar el modelo y el tokenizador\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"  # Este modelo ofrece puntuaciones\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Función para calcular la puntuación de sentimiento\n",
        "def sentiment_score(review):\n",
        "    tokens = tokenizer.encode(review, return_tensors='pt')\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits)) - 2\n",
        "\n",
        "# Aplicar la función a la columna \"Comentario\"\n",
        "df['SentimientoComentario'] = df['Comentario'].apply(sentiment_score)\n",
        "\n",
        "# Normalizar las puntuaciones al rango deseado (-1 a 1)\n",
        "df['SentimientoComentario'] = df['SentimientoComentario'] / 2.0\n",
        "\n",
        "# Muestra el DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "rwc3tUsVLq8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Muestra el DataFrame con una nueva columna 'SentimientoComentario' que refleja el análisis de sentimientos de los comentarios."
      ],
      "metadata": {
        "id": "N8DRor6Lcp8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reemplazando todos los puntos** ('.') por comas (',') en los datos, y luego guarda el DataFrame modificado en un nuevo archivo CSV."
      ],
      "metadata": {
        "id": "voE_48YQc_jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiar todos los puntos ('.') por comas (',') en el DataFrame\n",
        "df = df.astype(str).replace('\\.', ',', regex=True)\n",
        "# Guardar el DataFrame modificado en un nuevo archivo CSV\n",
        "df.to_csv('archivo_listo.csv', index=False, sep=';')"
      ],
      "metadata": {
        "id": "jtyMlrJNc_4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Realiza un cambio de formato en los datos, reemplazando los puntos por comas, lo que es útil para la normalización de datos o para cumplir con ciertos estándares de formato que requiere power bi, ademas crea el archivo CSV Final, el cual obtiene el merge de los 3 archivos CSV iniciales."
      ],
      "metadata": {
        "id": "h1EvLsbVdEbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modelo Modelo Modelo MOdelol modleo**"
      ],
      "metadata": {
        "id": "tVjbq93ztzSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procesar y preparar datos para análisis y modelado predictivo**, en el contexto de aprendizaje automático con Python. Se utiliza la biblioteca sklearn para varias transformaciones de datos:\n",
        "\n",
        "**Lectura y Preprocesamiento de Datos:** Lee datos de un archivo CSV, reemplazando las comas por puntos en ciertas columnas para asegurar que los datos sean numéricos.\n",
        "\n",
        "**Normalización de Características Numéricas:** Se aplica un escalado estándar a las características numéricas ('Puntuacion', 'SentimientoComentario').\n",
        "\n",
        "**Codificación de Variables Categóricas:** Conviertir categorías textuales en variables numéricas a través de la técnica de codificación one-hot. Esto permite que los algoritmos de aprendizaje automático procesen eficientemente estas características.\n",
        "\n",
        "**Preparación de Datos de Texto para NLP** (Procesamiento de Lenguaje Natural): Realiza una limpieza básica en los textos, como convertir a minúsculas y eliminar la puntuación."
      ],
      "metadata": {
        "id": "QbElKefRCd24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Lectura de datos desde un archivo CSV\n",
        "data = pd.read_csv('/content/archivo_listo.csv', delimiter=';')\n",
        "\n",
        "# Convertir Puntuacion y SentimientoComentario a numérico (reemplazando comas por puntos)\n",
        "data['Puntuacion'] = pd.to_numeric(data['Puntuacion'].str.replace(',', '.'))\n",
        "data['SentimientoComentario'] = pd.to_numeric(data['SentimientoComentario'].str.replace(',', '.'))\n",
        "\n",
        "# 2. Normalización de Características Numéricas\n",
        "# Lista de características numéricas\n",
        "numeric_features = ['Puntuacion', 'SentimientoComentario']\n",
        "\n",
        "# 3. Codificación de Variables Categóricas\n",
        "# Lista de características categóricas\n",
        "categorical_features = ['Gender', 'Place', 'Location', 'Type']\n",
        "\n",
        "# Construir un preprocesador compuesto para realizar la normalización y codificación\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Preparación de Datos de Texto para NLP\n",
        "# Limpieza básica de texto\n",
        "data['Comentario'] = data['Comentario'].str.lower()  # convertir a minúsculas\n",
        "data['Comentario'] = data['Comentario'].str.replace('[^\\w\\s]', '')  # eliminar puntuación\n",
        "\n",
        "# Aplicar el preprocesador a los datos (sin manejo de valores faltantes)\n",
        "data_preprocessed = preprocessor.fit_transform(data)\n",
        "\n",
        "# Mostrar los primeros 5 registros después de la preparación\n",
        "print(data_preprocessed[:5])"
      ],
      "metadata": {
        "id": "2vGQOJbPt2An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultadao Anterior** Se presentan en un formato de matriz dispersa (sparse matrix), donde cada fila representa un registro, y cada columna un atributo transformado. Por ejemplo, (0, 0) 0.46880723093849525 indica que en la primera fila, la primera característica numérica (probablemente 'Puntuacion') ha sido normalizada a 0.4688.\n",
        "\n",
        "Las características numéricas aparecen primero (escaladas), seguidas de las características categóricas (codificadas one-hot). Los valores 1.0 en distintas columnas indican la presencia de una categoría específica para esa fila, según la codificación one-hot."
      ],
      "metadata": {
        "id": "uRgogQj0G4N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proceso de preparación de datos para un modelo de recomendación**  o análisis en el ámbito del Procesamiento de Lenguaje Natural (NLP). Se centra en la transformación de textos (comentarios) a un formato numérico que pueda ser procesado por algoritmos de aprendizaje automático. Utiliza la técnica de Vectorización TF-IDF para convertir los textos en vectores numéricos, reflejando la importancia de cada palabra en el conjunto de datos. Luego, estas características textuales se combinan con otras características previamente procesadas (numéricas y categóricas), preparando un conjunto de datos integral para el modelado."
      ],
      "metadata": {
        "id": "D9hM3BcdIsNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Tokenización y Limpieza de Texto\n",
        "# Ya se realizó una limpieza básica en los pasos anteriores\n",
        "\n",
        "# 2. Vectorización de Texto\n",
        "# Convertir comentarios a formato numérico utilizando TF-IDF.\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "text_features = tfidf_vectorizer.fit_transform(data['Comentario'])\n",
        "\n",
        "# 3. Integración de Características Textuales con Otras Características\n",
        "# Combinar las características de texto (TF-IDF) con las características preprocesadas anteriores\n",
        "import scipy.sparse as sp\n",
        "final_features = sp.hstack((data_preprocessed, text_features))\n",
        "\n",
        "# Mostrar las dimensiones de las características finales\n",
        "final_features.shape\n"
      ],
      "metadata": {
        "id": "eBCe8tcwvZdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** El resultado final_features.shape muestra que el conjunto final de datos tiene 486 registros y 138 características. Esto significa que después de procesar los comentarios con TF-IDF y combinarlos con las características preprocesadas, se obtuvo una matriz con 486 filas (una por cada registro en el dataset) y 138 columnas (características)."
      ],
      "metadata": {
        "id": "rzJJjRt2J-Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasificador de Bosque Aleatorio** Proceso de construcción y entrenamiento de un modelo de clasificación, para predecir etiquetas o categorías (denominadas 'Label') en un conjunto de datos. El proceso implica dividir los datos en conjuntos de entrenamiento y prueba, entrenar el modelo con el conjunto de entrenamiento, y luego evaluar su rendimiento en el conjunto de prueb"
      ],
      "metadata": {
        "id": "YnSTJAHzSNN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. División de Datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features, data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Selección del Modelo\n",
        "# Usaremos un clasificador de bosque aleatorio, que es un buen punto de partida para problemas de clasificación\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 3. Entrenamiento del Modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Evaluación del Modelo\n",
        "# Realizamos predicciones en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluación del rendimiento del modelo\n",
        "performance_report = classification_report(y_test, y_pred)\n",
        "\n",
        "performance_report"
      ],
      "metadata": {
        "id": "S_zKdP4Qv3cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Obtenido**\n",
        "\n",
        "**Precisión y Recall Variados:** Las categorías '1' y '2' tienen valores de 0.00 en todas las métricas, lo que sugiere que el modelo no predijo correctamente ninguna muestra de estas clases.\n",
        "\n",
        "**Mejor Rendimiento en Categorías con Más Datos:** Las categorías con más muestras (como '5') tienen una mejor precisión y recall.\n",
        "\n",
        "**Exactitud General del 55%:** Indica que el modelo es correcto en aproximadamente la mitad de los casos.\n",
        "\n",
        "**F1-Score Variado:** Refleja un equilibrio entre la precisión y el recall, mostrando variaciones significativas entre categorías."
      ],
      "metadata": {
        "id": "c4ev3-EeS4W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construir y evaluar un modelo de clasificación** utilizando un Clasificador de Bosque Aleatorio (Random Forest Classifier). Este tipo de modelo es una técnica de aprendizaje supervisado que opera construyendo múltiples árboles de decisión durante el entrenamiento y produciendo la clase que es la moda de las clasificaciones de los árboles individuales para la clasificación."
      ],
      "metadata": {
        "id": "EPBBbyhFVDxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Dividimos el conjunto de datos en entrenamiento y prueba para poder evaluar el modelo\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features, data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializamos el clasificador de bosque aleatorio y lo entrenamos con los datos de entrenamiento\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Realizamos predicciones con el modelo en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "# Generamos un informe con las principales métricas de clasificación para evaluar el modelo\n",
        "performance_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Calculamos métricas adicionales para tener una evaluación más detallada del rendimiento del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Creamos y visualizamos la matriz de confusión para entender mejor dónde el modelo está haciendo correctas e incorrectas predicciones\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualización de la Matriz de Confusión con Seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicciones')\n",
        "plt.ylabel('Valores Verdaderos')\n",
        "plt.show()\n",
        "\n",
        "# Imprimimos todas las métricas calculadas para el modelo\n",
        "performance_report, accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "x577jtni6KR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** El modelo tiene problemas para clasificar las clases '1' y '2', ya que no ha hecho ninguna predicción correcta para estas clases, como se muestra por la precisión, el recall y el F1-score de 0.00.\n",
        "La clase '5' tiene la mayor cantidad de predicciones correctas, lo que puede ser debido a un mayor número de muestras de entrenamiento, lo que generalmente mejora el rendimiento del modelo para esa clase específica.\n",
        "Las clases '3' y '4' muestran un rendimiento intermedio, con algunas predicciones correctas, pero también con un número considerable de confusiones entre ellas.\n",
        "La exactitud global del modelo es del 55%, lo que indica que puede predecir correctamente más de la mitad de las veces en el conjunto de prueba.\n",
        "La matriz de confusión visual muestra que hay una tendencia del modelo a predecir con mayor frecuencia las clases con más ejemplos, lo cual es un comportamiento típico en conjuntos de datos desequilibrados."
      ],
      "metadata": {
        "id": "nw6ZPds2YKYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Realiza una búsqueda en cuadrícula con validación cruzada** para optimizar los hiperparámetros de un modelo de clasificación. El objetivo es encontrar la combinación de hiperparámetros (max_depth, min_samples_split, n_estimators) que resulte en la mejor precisión (accuracy) del modelo. Utiliza GridSearchCV de sklearn para automatizar la búsqueda y la validación cruzada, probando diferentes combinaciones de hiperparámetros y validando cada combinación mediante el método de validación cruzada con 5 particiones."
      ],
      "metadata": {
        "id": "yV7UsfzBYmza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esto, utilizaremos una técnica llamada búsqueda en cuadrícula (Grid Search) con validación cruzada, que prueba sistemáticamente una serie de combinaciones de hiperparámetros y evalúa su rendimiento."
      ],
      "metadata": {
        "id": "nm1nQP-xxBIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definición de los hiperparámetros a ajustar\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Define un rango para la cantidad de árboles en el bosque\n",
        "    'max_depth': [None, 10, 20, 30],  # Define un rango para la máxima profundidad de cada árbol\n",
        "    'min_samples_split': [2, 5, 10]  # Define el número mínimo de muestras requeridas para dividir un nodo\n",
        "}\n",
        "\n",
        "# Creación del objeto GridSearchCV para realizar la búsqueda en cuadrícula con los parámetros definidos\n",
        "# 'cv=5' indica que se usa validación cruzada de 5 folds\n",
        "# 'n_jobs=-1' permite usar todos los procesadores disponibles para paralelizar la operación\n",
        "# 'scoring='accuracy'' establece la precisión como la métrica para evaluar el rendimiento\n",
        "ggrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Ejecución de la búsqueda en cuadrícula en los datos de entrenamiento para encontrar los mejores hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtención de los mejores hiperparámetros y la mejor puntuación de precisión después de la búsqueda en cuadrícula\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Los resultados muestran los mejores hiperparámetros encontrados y la precisión correspondiente\n",
        "best_params, best_score"
      ],
      "metadata": {
        "id": "QAUgym3-xCJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Los resultados obtenidos del GridSearchCV indican que la mejor configuración de hiperparámetros para el clasificador de bosque aleatorio en este caso es con max_depth no definido (lo que permite que los árboles crezcan tanto como sea necesario), min_samples_split en 10 (requiriendo al menos 10 muestras para dividir un nodo interno), y n_estimators en 50 (usando 50 árboles en el bosque). La mejor puntuación de precisión obtenida con esta configuración es aproximadamente 0.634, lo que significa que el modelo, con estos hiperparámetros, tiene una precisión del 63.4% en la clasificación correcta de las muestras durante la validación cruzada."
      ],
      "metadata": {
        "id": "gxeI3PTXby9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reentrenar un modelo de clasificación de bosque aleatorio** (Random Forest) utilizando hiperparámetros que han sido identificados como óptimos mediante un proceso de búsqueda en cuadrícula (GridSearchCV). Una vez reentrenado, el modelo se evalúa con el conjunto de prueba para determinar si la optimización de hiperparámetros ha mejorado su rendimiento en términos de precisión, recall y F1-score."
      ],
      "metadata": {
        "id": "KP5QFbPhcapq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo reentrenado con los hiperparámetros óptimos ha sido evaluado en el conjunto de prueba."
      ],
      "metadata": {
        "id": "Upl3pQBTzEef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reentrenamiento del modelo con los hiperparámetros óptimos encontrados por GridSearchCV\n",
        "model_optimized = RandomForestClassifier(\n",
        "    n_estimators=50, # Número óptimo de árboles\n",
        "    max_depth=None, # Profundidad máxima de los árboles sin restricciones\n",
        "    min_samples_split=10, # Número mínimo de muestras requeridas para dividir un nodo\n",
        "    random_state=42 # Semilla para la reproducibilidad de los resultados\n",
        ")\n",
        "\n",
        "# Entrenamiento del modelo optimizado con el conjunto de entrenamiento\n",
        "model_optimized.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones con el modelo optimizado en el conjunto de prueba\n",
        "y_pred_optimized = model_optimized.predict(X_test)\n",
        "\n",
        "# Generación del informe de rendimiento para el modelo optimizado\n",
        "# Esto incluye precisión, recall y F1-score para cada una de las clases\n",
        "performance_report_optimized = classification_report(y_test, y_pred_optimized)\n",
        "performance_report_optimized"
      ],
      "metadata": {
        "id": "eVMfY4BHzDfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior**\n",
        "La precisión global ponderada y el F1-score de 0.46 y 0.51, respectivamente, junto con una precisión total del 59%, sugieren que el modelo optimizado ha mejorado en comparación con el modelo antes de la optimización de hiperparámetros.\n",
        "\n",
        "Este análisis sugiere que, aunque la optimización de hiperparámetros ha mejorado el rendimiento general del modelo, todavía hay deficiencias significativas en la capacidad del modelo para clasificar todas las clases de manera equitativa."
      ],
      "metadata": {
        "id": "3GcDL5H6cz_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mtraiz Confusion**"
      ],
      "metadata": {
        "id": "OTJckhx_gLKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculamos las métricas de rendimiento para el modelo optimizado\n",
        "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
        "precision_optimized = precision_score(y_test, y_pred_optimized, average='weighted')\n",
        "recall_optimized = recall_score(y_test, y_pred_optimized, average='weighted')\n",
        "f1_optimized = f1_score(y_test, y_pred_optimized, average='weighted')\n",
        "\n",
        "# Creación de la Matriz de Confusión para el modelo optimizado\n",
        "conf_matrix_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
        "\n",
        "# Visualización de la Matriz de Confusión\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_optimized, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Matriz de Confusión del Modelo Optimizado')\n",
        "plt.xlabel('Predicciones')\n",
        "plt.ylabel('Valores Verdaderos')\n",
        "plt.show()\n",
        "\n",
        "accuracy_optimized, precision_optimized, recall_optimized, f1_optimized\n"
      ],
      "metadata": {
        "id": "pwB4H6wa5oyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explorar técnicas de procesamiento de NLP para extraer características más significativas del texto.**"
      ],
      "metadata": {
        "id": "ujTPJaihdi7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "profundizaremos en la extracción de características mejorada utilizando técnicas más avanzadas de procesamiento de NLP. El objetivo es capturar de manera más efectiva la información contenida en los comentarios y descripciones, lo cual podría mejorar significativamente el rendimiento del modelo de recomendación. Aquí están los enfoques que exploraremos:"
      ],
      "metadata": {
        "id": "xmXictGPzgwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El propósito es profundizar en la comprensión del lenguaje y extraer características más ricas y significativas de los comentarios y descripciones. Las técnicas avanzadas como BERT, GPT, análisis de sentimientos, descomposición de tópicos y entrenamiento de word embeddings personalizados permiten capturar la semántica, el contexto, las emociones y los temas clave de los textos, lo cual puede incrementar significativamente la eficacia de un modelo de recomendación."
      ],
      "metadata": {
        "id": "DRbeNSG4gsKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Está diseñado para generar representaciones vectoriales (embeddings) de texto utilizando BERT (Bidirectional Encoder Representations from Transformers), un modelo de procesamiento de lenguaje natural (NLP) preentrenado. Estas representaciones capturan el contexto semántico de las palabras en los comentarios y pueden mejorar significativamente las características utilizadas para el entrenamiento de modelos de aprendizaje automático,"
      ],
      "metadata": {
        "id": "CkDiDuYPhZvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Cargar el tokenizador y modelo preentrenado BERT base version no diferenciador de mayúsculas\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def bert_embeddings(text):\n",
        "    \"\"\"\n",
        "    Función para obtener embeddings de BERT para un texto dado.\n",
        "    \"\"\"\n",
        "    # Tokeniza el texto y lo prepara como input para el modelo BERT\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Pasa los inputs tokenizados al modelo BERT y obtiene los outputs\n",
        "    outputs = model_bert(**inputs)\n",
        "\n",
        "    # Retorna el promedio de los últimos estados ocultos como la representación vectorial del texto\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Aplica la función a cada comentario para obtener sus embeddings y los almacena en un tensor\n",
        "bert_features = torch.stack([torch.tensor(bert_embeddings(comment)) for comment in data['Comentario']])\n",
        "\n",
        "# Muestra las dimensiones del tensor de features, indicando el número de comentarios y la longitud del embedding\n",
        "bert_features.shape"
      ],
      "metadata": {
        "id": "msCY0DYjzfd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resutado Anterior** Los resultados de la ejecución del código muestran que los embeddings de BERT se han calculado con éxito para 486 comentarios, con cada comentario siendo representado por un vector de 768 elementos (el tamaño de los embeddings de la capa oculta de BERT base). El tamaño del tensor resultante es [486, 1, 768], lo que sugiere que ahora tenemos una representación densa y rica en información para cada comentario, que podría ser utilizada para entrenar un modelo de clasificación o regresión."
      ],
      "metadata": {
        "id": "bw1Pz98Fh8Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente paso es integrarlas con tus características numéricas y\n",
        "categóricas preprocesadas y luego utilizar este conjunto de características combinado para entrenar y evaluar tu modelo de recomendación."
      ],
      "metadata": {
        "id": "Jl1ZYqmO0_l5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mejorar el conjunto de características para el entrenamiento de un modelo** de clasificación incorporando representaciones de texto avanzadas obtenidas a través de BERT. Al combinar estas representaciones de lenguaje natural con características numéricas y categóricas preprocesadas, se busca enriquecer el conjunto de datos y potencialmente aumentar la precisión del modelo al capturar mejor la semántica y el contexto de los comentarios."
      ],
      "metadata": {
        "id": "lYu3RntBkNvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convertir los embeddings de BERT a un array de NumPy y remover la dimensión extra\n",
        "bert_features_np = bert_features.squeeze().numpy()\n",
        "\n",
        "# Combinar los embeddings de BERT con las características preprocesadas convirtiéndolas a NumPy array\n",
        "combined_features = np.hstack((data_preprocessed.toarray(), bert_features_np))\n",
        "\n",
        "# División del conjunto de datos combinados en entrenamiento y prueba\n",
        "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(\n",
        "    combined_features,\n",
        "    data['Label'], # Asumiendo que 'data' es un DataFrame y 'Label' es la columna objetivo\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Inicialización y entrenamiento del modelo de clasificación con las características combinadas\n",
        "model_optimized.fit(X_train_combined, y_train_combined)\n",
        "\n",
        "# Predicción y evaluación del modelo con el conjunto de prueba\n",
        "y_pred_combined = model_optimized.predict(X_test_combined)\n",
        "performance_report_combined = classification_report(y_test_combined, y_pred_combined)\n",
        "\n",
        "performance_report_combined"
      ],
      "metadata": {
        "id": "O7L8NFAu09bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Anterior** Después de combinar las características textuales avanzadas con las características numéricas y categóricas preprocesadas, el modelo de clasificación optimizado presenta los siguientes resultados:\n",
        "\n",
        "Las clases '1', '2', y '3' tienen una precisión y recall de 0.00. Esto sugiere que el modelo no está clasificando correctamente estas clases. La falta de rendimiento puede deberse a una muestra insuficiente de estas clases en los datos de entrenamiento, características poco representativas, o puede que el modelo necesite una estructura más compleja o diferente para capturar la variabilidad en estas clases específicas.\n",
        "\n",
        "La clase '4' muestra una mejora en la precisión al 0.50 y un recall de 0.33. Aunque hay un avance en comparación con las clases '1' a '3', todavía indica un margen de mejora significativo para predecir correctamente estas muestras.\n",
        "\n",
        "La clase '5' mantiene un rendimiento relativamente alto, con una precisión de 0.57 y un recall de 0.92, lo que resulta en un F1-score de 0.70. Esto indica que el modelo es capaz de identificar y clasificar con precisión las muestras de esta clase.\n",
        "\n",
        "La precisión ponderada y el F1-score son 0.42 y 0.46 respectivamente, con una exactitud global de 0.55. Comparado con los resultados previos al uso de BERT, parece haber una disminución en la exactitud general, lo que podría indicar que la integración de características de BERT podría necesitar un ajuste adicional o que otras características están influyendo en el desempeño de las clases minoritarias.\n",
        "\n",
        "La conclusión es que, aunque la integración de BERT ha enriquecido el conjunto de características, no necesariamente se traduce en una mejora directa en todas las métricas de clasificación."
      ],
      "metadata": {
        "id": "iH8dS5lzkvxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metricas *1* Aplicar Métricas al Modelo Propuesto**"
      ],
      "metadata": {
        "id": "3FFk9Uiv2uxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculamos las métricas de rendimiento\n",
        "accuracy = accuracy_score(y_test_combined, y_pred_combined)\n",
        "precision = precision_score(y_test_combined, y_pred_combined, average='weighted')\n",
        "recall = recall_score(y_test_combined, y_pred_combined, average='weighted')\n",
        "f1 = f1_score(y_test_combined, y_pred_combined, average='weighted')\n",
        "\n",
        "# Creación de la Matriz de Confusión\n",
        "conf_matrix = confusion_matrix(y_test_combined, y_pred_combined)\n",
        "\n",
        "# Visualización de la Matriz de Confusión\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicciones')\n",
        "plt.ylabel('Valores Verdaderos')\n",
        "plt.show()\n",
        "\n",
        "# Resultados de las métricas\n",
        "accuracy, precision, recall, f1\n",
        "\n"
      ],
      "metadata": {
        "id": "ccLRvcDn2wO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretación de la Matriz de Confusión:\n",
        "Filas (Valores Verdaderos): Representan las clases reales de los datos de prueba.\n",
        "Columnas (Predicciones): Representan las clases predichas por el modelo.\n",
        "Diagonal Principal: Muestra el número de predicciones correctas para cada clase (verdaderos positivos).\n",
        "Fuera de la Diagonal Principal: Representa los casos en los que el modelo ha hecho una predicción incorrecta (falsos positivos y falsos negativos).\n",
        "Observaciones específicas:\n",
        "\n",
        "El modelo predijo correctamente 45 instancias de la clase '5', lo que sugiere que es la clase con mejor desempeño.\n",
        "Para la clase '4', el modelo predijo correctamente 9 instancias, pero hubo 4 instancias que el modelo clasificó incorrectamente como '5'.\n",
        "Las clases '1', '2' y '3' tienen muy pocas o ninguna predicción correcta, lo que indica que el modelo tiene dificultades con estas clases. Esto puede ser un indicador de desbalance de clases en el conjunto de datos.\n",
        "Interpretación de las Métricas:\n",
        "Exactitud (Accuracy): Aproximadamente el 55.10% de todas las predicciones fueron correctas.\n",
        "Precisión (Precision): Cuando el modelo predice una clase, es correcto aproximadamente el 42.26% del tiempo.\n",
        "Recall: El modelo identifica correctamente aproximadamente el 55.10% de todas las instancias positivas.\n",
        "F1-Score: Es una medida de la precisión y el recall. En este caso, es aproximadamente 0.4617, lo que sugiere que hay un equilibrio relativamente bajo entre la precisión y el recall del modelo.\n",
        "Conclusiones:\n",
        "El modelo muestra un buen rendimiento al predecir la clase '5', pero es menos efectivo con las demás clases.\n",
        "El bajo rendimiento en las clases '1', '2' y '3' puede deberse a un desbalance en el conjunto de datos, donde estas clases pueden estar subrepresentadas.\n",
        "La precisión y el F1-Score bajos sugieren que hay margen de mejora, particularmente en la capacidad del modelo para manejar clases menos representadas o más difíciles de diferenciar."
      ],
      "metadata": {
        "id": "AA6FvNgs3pHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicciones con el conjunto de prueba**"
      ],
      "metadata": {
        "id": "vglWhEipLsVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones con el conjunto de prueba\n",
        "y_pred_test = model_optimized.predict(X_test_combined)\n",
        "\n",
        "# Análisis de los resultados de la prueba\n",
        "# Comparación de las predicciones con los valores reales\n",
        "print(\"Predicciones:\", y_pred_test)\n",
        "print(\"Valores verdaderos:\", y_test_combined)\n",
        "\n",
        "# Obtener una visión general del rendimiento del modelo en cada clase\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_combined, y_pred_test))\n"
      ],
      "metadata": {
        "id": "GGl5Vjj63vnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicciones vs. Valores Verdaderos:\n",
        "\n",
        "Las predicciones del modelo están fuertemente sesgadas hacia las clases '4' y '5', con una tendencia a predecir '5' la mayoría de las veces.\n",
        "Los valores verdaderos muestran una distribución más uniforme entre las clases '3', '4' y '5', con algunas instancias pertenecientes a las clases '1' y '2'.\n",
        "Métricas de Rendimiento:\n",
        "\n",
        "Precisión: Para las clases '4' y '5', es del 50% y 57%, respectivamente, lo que significa que cuando el modelo predice estas clases, es correcto la mitad del tiempo para '4' y algo más de la mitad para '5'.\n",
        "Recall: El modelo tiene un recall de 0% para las clases '1', '2' y '3', lo que significa que no identificó correctamente ninguna de las instancias verdaderas para estas clases. Para las clases '4' y '5', los valores de recall son del 33% y 92%, respectivamente, lo que indica que el modelo es capaz de identificar la mayoría de las instancias de la clase '5', pero no así para la clase '4'.\n",
        "F1-Score: Los valores de F1-Score son bajos para todas las clases excepto para la '5'. El F1-Score es una media armónica de la precisión y el recall, y un valor bajo indica que el modelo no está equilibrado en términos de precisión y recall.\n",
        "Exactitud (Accuracy):\n",
        "\n",
        "La exactitud global es del 55%, lo que indica que más de la mitad de las predicciones totales fueron correctas. Sin embargo, esta métrica puede ser engañosa debido al desbalance entre las clases.\n",
        "Inferencias y Conclusiones:\n",
        "El modelo está claramente sesgado hacia las clases con más datos, particularmente la clase '5'. Esto puede ser un resultado del desbalance de clases en el conjunto de datos de entrenamiento.\n",
        "La incapacidad del modelo para identificar las clases '1', '2' y '3' sugiere que no está aprendiendo características discriminativas para estas clases o que hay muy pocos datos para aprender de estas clases.\n",
        "A pesar de que la clase '4' está mejor representada que '1', '2' y '3', el modelo aún lucha por predecirla correctamente, lo que podría ser un indicativo de características compartidas o confusas entre las clases '4' y '5'.\n",
        "La métrica de exactitud no es un buen indicador del rendimiento del modelo en este caso, debido a la distribución desigual de las clases."
      ],
      "metadata": {
        "id": "RvRo9oUn4w_5"
      }
    }
  ]
}